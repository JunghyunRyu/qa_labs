# QA-Arena 사용자 가이드

QA-Arena 플랫폼 사용 방법을 안내합니다.

## 목차

1. [시작하기](#시작하기)
2. [문제 풀기](#문제-풀기)
3. [제출 및 결과 확인](#제출-및-결과-확인)
4. [AI 피드백 이해하기](#ai-피드백-이해하기)
5. [Admin 기능 (관리자)](#admin-기능-관리자)
6. [자주 묻는 질문](#자주-묻는-질문)

## 시작하기

### 1. 플랫폼 접속

웹 브라우저에서 QA-Arena 플랫폼에 접속하세요.

- 개발 환경: `http://localhost:3000`
- 프로덕션 환경: 배포된 도메인 주소

### 2. 문제 목록 확인

메인 페이지에서 사용 가능한 문제 목록을 확인할 수 있습니다. 각 문제 카드에는 다음 정보가 표시됩니다:

- 문제 제목
- 난이도 (Easy, Medium, Hard)
- 평가할 스킬 목록

## 문제 풀기

### 1. 문제 선택

문제 목록에서 원하는 문제를 클릭하여 상세 페이지로 이동합니다.

### 2. 문제 이해하기

문제 상세 페이지에서 다음 정보를 확인하세요:

- **문제 설명**: Markdown 형식으로 작성된 문제 설명
- **함수 시그니처**: 테스트해야 할 함수의 시그니처
- **초기 템플릿**: 테스트 코드 작성을 위한 기본 템플릿

### 3. 테스트 코드 작성

Monaco Editor를 사용하여 pytest 기반 테스트 코드를 작성하세요.

#### 테스트 코드 작성 가이드

1. **기본 구조**:
```python
import pytest
from target import function_name

def test_basic_case():
    """기본 케이스 테스트"""
    result = function_name([1, 2, 3])
    assert result == 6

def test_edge_case():
    """경계값 테스트"""
    result = function_name([]))
    assert result == 0
```

2. **테스트 케이스 작성 팁**:
   - 정상 케이스: 일반적인 입력에 대한 테스트
   - 경계값 케이스: 빈 리스트, 0, 음수 등
   - 예외 케이스: 잘못된 입력에 대한 처리
   - 다양한 시나리오: 가능한 많은 케이스를 커버

3. **주의사항**:
   - 함수 시그니처를 정확히 따라야 합니다
   - `target` 모듈에서 함수를 import해야 합니다
   - 테스트 함수는 `test_`로 시작해야 합니다

## 제출 및 결과 확인

### 1. 코드 제출

작성한 테스트 코드를 검토한 후 "제출" 버튼을 클릭하세요.

### 2. 채점 진행 상황

제출 후 다음 상태를 확인할 수 있습니다:

- **대기 중**: 채점 대기 중
- **채점 중...**: 현재 채점 진행 중
- **성공**: 채점 완료
- **실패**: 테스트 실패
- **오류 발생**: 시스템 오류

### 3. 결과 확인

채점이 완료되면 다음 정보를 확인할 수 있습니다:

- **점수**: 0-100점 사이의 점수
- **뮤턴트 제거율**: 작성한 테스트가 얼마나 많은 버그를 찾았는지
- **실행 로그**: 테스트 실행 결과 상세 정보

## AI 피드백 이해하기

### 피드백 구성 요소

AI 피드백은 다음 섹션으로 구성됩니다:

1. **요약**: 전체적인 평가 요약
2. **잘한 점**: 잘 작성된 부분
3. **아쉬운 점**: 개선이 필요한 부분
4. **제안된 테스트**: 추가로 작성하면 좋을 테스트 케이스
5. **점수 조정**: 피드백 기반 점수 조정 (있는 경우)

### 피드백 활용하기

- 피드백을 바탕으로 테스트 코드를 개선하세요
- 제안된 테스트 케이스를 참고하여 추가 테스트를 작성하세요
- 아쉬운 점을 개선하여 재제출하세요

## Admin 기능 (관리자)

### AI 문제 생성

관리자는 AI를 활용하여 새로운 문제를 생성할 수 있습니다.

#### 1. 문제 생성 페이지 접속

`/admin/problems/new` 경로로 이동하세요.

#### 2. 문제 생성 요청

다음 정보를 입력하세요:

- **목표 (Goal)**: 문제의 목표 설명
- **평가할 스킬 (Skills to Assess)**: 평가하고자 하는 스킬 목록
- **난이도 (Difficulty)**: Easy, Medium, Hard 중 선택
- **언어 (Language)**: Python (기본값)
- **테스트 프레임워크 (Testing Framework)**: pytest (기본값)
- **문제 스타일 (Problem Style)**: algorithm, data_structure 등

#### 3. 생성된 문제 검토

AI가 생성한 문제를 검토하고 필요한 경우 수정하세요.

#### 4. 문제 저장

검토가 완료되면 "문제 저장" 버튼을 클릭하여 데이터베이스에 저장하세요.

## 자주 묻는 질문

### Q: 테스트 코드가 실행되지 않아요

A: 다음 사항을 확인하세요:
- 함수 시그니처가 정확한지 확인
- `target` 모듈에서 올바르게 import했는지 확인
- 테스트 함수 이름이 `test_`로 시작하는지 확인

### Q: 점수가 낮게 나왔어요

A: 다음을 시도해보세요:
- 더 많은 테스트 케이스를 작성
- 경계값 케이스 추가
- AI 피드백을 참고하여 개선

### Q: 뮤턴트 제거율이 낮아요

A: 뮤턴트 제거율을 높이려면:
- 다양한 입력 케이스를 테스트
- 엣지 케이스와 예외 상황을 커버
- 테스트의 범위를 넓히기

### Q: AI 피드백이 생성되지 않아요

A: 다음을 확인하세요:
- OpenAI API 키가 올바르게 설정되었는지 확인
- 채점이 완료되었는지 확인
- 네트워크 연결 상태 확인

### Q: 문제를 다시 풀 수 있나요?

A: 네, 같은 문제에 여러 번 제출할 수 있습니다. 각 제출은 독립적으로 채점됩니다.

## 지원

문제가 발생하거나 도움이 필요한 경우:

1. 문제를 상세히 설명하여 이슈를 등록하세요
2. 로그 파일을 확인하여 오류 정보를 수집하세요
3. 관리자에게 문의하세요

## 추가 리소스

- [pytest 문서](https://docs.pytest.org/)
- [Python 테스트 가이드](https://docs.python.org/3/library/unittest.html)
- [QA-Arena 개발자 가이드](CONTRIBUTING.md)

